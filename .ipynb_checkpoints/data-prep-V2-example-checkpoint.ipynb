{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf506345-1374-4806-b8d9-918312687f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79e05c35-1132-4ef9-b705-c5a8562d806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --user datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84884a32-9287-48a8-b8ca-a386c0079280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57c856b-d89f-4f62-8cc3-357bfc2e1a8c",
   "metadata": {},
   "source": [
    "### 1. Identify Your Domain-Specific Dataset\n",
    "- Choose a dataset related to a particular domain (e.g., legal, medical, finance, science, etc.).\n",
    "- Ensure that the dataset contains enough textual data to be useful for training an MLM but not so much that you can't process it (may need to be some trial and error).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14641d28-800b-4c5c-9b6a-abe5c262c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Cleaned_Medical_Text_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "807af1cf-19b7-49a0-b6d9-d4b428d91bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Medical Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The subject showed signs of pancytopenia, a me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An increase in C-reactive protein (CRP) levels...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The PET procedure revealed hyperintense T2-wei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A thorough differential evaluation was perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Patients with acute myocardial infarction (AMI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Medical Text\n",
       "0  The subject showed signs of pancytopenia, a me...\n",
       "1  An increase in C-reactive protein (CRP) levels...\n",
       "2  The PET procedure revealed hyperintense T2-wei...\n",
       "3  A thorough differential evaluation was perform...\n",
       "4  Patients with acute myocardial infarction (AMI..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: do some nlp clean up on it\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71351d2c-1406-48a8-af98-417d47a45944",
   "metadata": {},
   "source": [
    "#### **Converting a Pandas DataFrame to a Hugging Face Dataset**\n",
    "The following line of code converts a Pandas DataFrame into a Hugging Face `Dataset`, \n",
    "which is optimized for use in HF training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef22bf62-14dd-4e16-be82-18bd9d31a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "##use Dataset from HF to make training easier later\n",
    "dataset = Dataset.from_pandas(pd.DataFrame({\"text\": df[\"Medical Text\"].tolist()}))\n",
    "##alternate datatypes\n",
    "# Dataset.from_dict()\n",
    "# Dataset.from_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d6fcfef-3166-4bae-a68e-2318b3d42a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98d08e9-22da-4dc5-b84a-41c78db51b70",
   "metadata": {},
   "source": [
    "### 3. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "804e5baf-64e9-4757-a583-f71ed6bce377",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##load models\n",
    "model_name = \"distilbert-base-uncased\" #\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df87b42-9389-4a64-9c26-8ffe4f2e5931",
   "metadata": {},
   "source": [
    "#### **Converting a Pandas DataFrame to a Hugging Face Dataset**\n",
    "The below code applies a tokenization function to an entire dataset using Hugging Face's Dataset.map() function. There are two parts: <br>\n",
    "**The tokenize_function tokenizes text data using a Hugging Face tokenizer**\n",
    "* Truncation (truncation=True) → Ensures long texts are cut off at 512 tokens (BERT’s max length).\n",
    "* Padding (padding=True) → Ensures shorter texts are padded to 512 tokens.\n",
    "* Max Length (max_length=512) → Defines the maximum token limit per input.\n",
    "  <br>\n",
    "\n",
    "**dataset.map applies the function to the entire dataset**\n",
    "\n",
    "* Lambda Function (lambda x: tokenize_function(x, tokenizer))\n",
    "* Converts dataset.map() into a format that passes tokenizer into tokenize_function().\n",
    "batched=True → Optimizes Processing\n",
    "\n",
    "* Instead of processing one row at a time, it processes multiple rows at once, which is faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8687fa77-332c-40e2-a9d3-d65b363e43a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples, tokenizer):\n",
    "    \"\"\"Tokenizes input text for MLM fine-tuning.\"\"\"\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de2794c1-9ad7-410d-b673-b7acb1b45e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 33413.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "##tokenize the dataset\n",
    "#The .map() tokenization only prepares the dataset—but does not return PyTorch tensors.\n",
    "tokenized_datasets = dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dabf348-3bae-47d7-b66c-a7d34605028b",
   "metadata": {},
   "source": [
    "#### **Using HF Data Collator**\n",
    "* A utility from Hugging Face's transformers library that helps prepare data batches for training models like BERT, RoBERTa, and DistilBERT.\n",
    "* It automatically masks words in the input so that the model can learn to predict the missing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67f66f4c-6e55-4de4-a0dc-62eb8eb18e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab77fc09-0414-4466-9814-31cbaad6f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, tokenizer, dataset, data_collator):\n",
    "    \"\"\"\n",
    "    Computes the perplexity of a dataset using a masked language model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The pre-trained language model (e.g., BERT, DistilBERT).\n",
    "    - tokenizer: The tokenizer corresponding to the model.\n",
    "    - dataset: A dictionary containing a list of text samples under the \"text\" key.\n",
    "    - data_collator: A collator that applies dynamic token masking for MLM training.\n",
    "\n",
    "    Returns:\n",
    "    - float: The computed perplexity value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure model is in evaluation mode (prevents gradient updates)\n",
    "    model.eval()\n",
    "\n",
    "    # List to store loss values for each sample\n",
    "    losses = []\n",
    "\n",
    "    # Iterate over each text sample in the dataset\n",
    "    for example in dataset[\"text\"]:\n",
    "        \n",
    "        # Convert the text input into tokenized format\n",
    "        # `return_tensors=\"pt\"` returns tensors for PyTorch\n",
    "        # `truncation=True` ensures long texts are truncated to max length\n",
    "        # `padding=True` ensures consistent input size\n",
    "        inputs = tokenizer(example, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "        # Error handlind: \n",
    "        # Check if `input_ids` tensor is empty (can happen for blank inputs)\n",
    "        if inputs[\"input_ids\"].nelement() == 0:\n",
    "            print(f\"Skipping empty input: {example}\")\n",
    "            continue\n",
    "\n",
    "        # Apply masking using the data collator\n",
    "        # The collator takes care of randomly masking tokens in the batch\n",
    "        masked_batch = data_collator([{\"input_ids\": inputs[\"input_ids\"].squeeze(0)}])        \n",
    "\n",
    "        # Move the masked batch tensors to the model's device (GPU if available)\n",
    "        masked_batch = {k: v.to(model.device) for k, v in masked_batch.items()}\n",
    "\n",
    "        # Disable gradient computation to save memory and improve speed\n",
    "        with torch.no_grad():\n",
    "            # Forward pass: compute predictions and loss\n",
    "            outputs = model(**masked_batch)\n",
    "\n",
    "            # Extract loss value from the model output\n",
    "            loss = outputs.loss.item()\n",
    "\n",
    "            # Debugging: Check if loss is NaN or Inf (should not happen)\n",
    "            if np.isnan(loss) or np.isinf(loss):\n",
    "                print(f\"Skipping invalid loss for input: {example}\")\n",
    "                continue  # Skip this sample\n",
    "            \n",
    "            # Store valid loss values for later averaging\n",
    "            losses.append(loss)\n",
    "\n",
    "    # Handle case where all samples were skipped (to prevent NaN output)\n",
    "    if not losses:\n",
    "        return np.nan  # Return NaN if no valid losses were recorded\n",
    "\n",
    "    # Compute perplexity:\n",
    "    # Perplexity = exp(mean_loss)  (Lower perplexity means better language modeling)\n",
    "    return np.exp(np.mean(losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d70a1ce-1ede-410d-b39b-769fa2fbcb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Perplexity: 13.32\n"
     ]
    }
   ],
   "source": [
    "perplexity_original = compute_perplexity(model, tokenizer, dataset, data_collator)\n",
    "print(f\"Baseline Perplexity: {perplexity_original:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cc1a5f-cd08-4cba-8662-c557407c661d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
